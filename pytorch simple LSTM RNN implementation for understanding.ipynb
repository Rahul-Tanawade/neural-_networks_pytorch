{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM5r8JSwF7z6rcJ8IoHp3jT"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5mMAPVeLJTi"
      },
      "source": [
        "we are trying to demonstarte simple LSTM based RNN implementaton here\n",
        "\n",
        "we have a sentence \"\"i love neural networks\"\n",
        "\n",
        "next we will design a model, such that if ip is \"i love neu\" \n",
        "op will be \"i love neur\"\n",
        "\n",
        "for demonstration we will only train on \"i love neural networks\"\n",
        "\n",
        "I’ve overfit the model by training on multiple iterations using the same sequence again and again. I’d like to highlight again that , this post is not aimed at teaching RNN or LSTM. It’s just a walk through of working implementation of character generation model LSTM RNN using pytorch python library\n",
        "\n",
        "while training :\n",
        "for LSTM model, \n",
        "we will give a sequence at a time and it will predict a next letter\n",
        "\n",
        "for example  torch.Size([22, 1, 28])\n",
        "\n",
        "above is shape of input, 22 is squence length and 28 vector length for each character.\n",
        "\n",
        "if input is :'i love neur'\n",
        "then  results of model are:\n",
        "\n",
        "out=predict('i love neur') #input shape is torch.Size([11, 1, 28])\n",
        "\n",
        "print(out)# a\n",
        "\n",
        "here ([11,1,28]), 11 is sequence length, 1 is batch size and 28 is vector representation of every char in sequence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1NTrnnHp2Az"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-f3epWKkZ0M",
        "outputId": "837bf521-036e-4485-954f-19479c170459"
      },
      "source": [
        "data=\"i love neural networks\"\n",
        "\n",
        "seq_len=len(data)\n",
        "print(seq_len)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHLIo8MplZKR",
        "outputId": "924e2230-1ce3-40bc-f0a5-39003ef9bb3b"
      },
      "source": [
        "import string\n",
        "letters = string.ascii_lowercase+' #'\n",
        "n_letters = len(letters)\n",
        "print('Letter set is '+letters)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Letter set is abcdefghijklmnopqrstuvwxyz #\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAfW8r2hnDda"
      },
      "source": [
        "#my neural network\n",
        "\n",
        "class LSTMNN(nn.Module):\n",
        "    def __init__(self,input_dim,hidden_dim):\n",
        "       super(LSTMNN,self).__init__()\n",
        "       self.input_dim=input_dim\n",
        "       self.hidden_dim=hidden_dim\n",
        "       ##LSTM takes, input dimensions, hidden dimensions and num layers in case of stacked LSTMs (Default is 1)\n",
        "       self.LSTM=nn.LSTM(input_dim,hidden_dim)\n",
        "\n",
        "\n",
        "#Input must be 3 dimensional (Sequence len, batch, input dimensions) \n",
        "#hc is a tuple which contains the vectors h (hidden/feedback) and c (cell state vector  )\n",
        "\n",
        "    def forward(self,inp,hc):\n",
        "     ##this gives outut for each input and also (hidden and cell state vector)\n",
        "     output,_=self.LSTM(inp,hc)\n",
        "     return output\n",
        "\n"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdsmpE6EoqAF"
      },
      "source": [
        "#Dimensions of output of neural network is (seq_len, batch , hidden_dim). Since we want output dimensions to be\n",
        "#the same as n_letters, hidden_dim = n_letters (output dimensions = hidden_dimensions)\n",
        "hidden_dim = n_letters\n",
        "\n",
        "#Invoking model. Input dimensions = n_letters i.e 28. output dimensions = hidden_dimensions = 28\n",
        "\n",
        "model = LSTMNN(n_letters,hidden_dim)\n",
        "\n",
        "#I'm using Adam optimizer here\n",
        "optimizer = torch.optim.Adam(params = model.parameters(),lr=0.01)\n",
        "\n",
        "#Loss function is CrossEntropyLoss\n",
        "LOSS = torch.nn.CrossEntropyLoss()"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiJJKTeLqgig"
      },
      "source": [
        "The model we are trying to build is a character generation model. That means, we will input the sequence , “i love neural networks” to the model. Since this is a supervised technique, we must have a output to train the model. I’m taking the output for a sequence is ,next letter in the sequence. That means, for the input sequence “i love neu”, output character must be “r”, since r is the next letter in the original sequence. Similarily, for input sequence “i love neura”, output sequence is “l”. Now. let’s try to build actual outputs. Let’s call them ‘targets’."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJMDYQ-5qgfp"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brnWq3axGTde",
        "outputId": "b657b61b-73d7-4e08-929a-42aa35d06713"
      },
      "source": [
        "print(data)\n",
        "for x in data[1:]+'#':\n",
        "  print(x)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i love neural networks\n",
            " \n",
            "l\n",
            "o\n",
            "v\n",
            "e\n",
            " \n",
            "n\n",
            "e\n",
            "u\n",
            "r\n",
            "a\n",
            "l\n",
            " \n",
            "n\n",
            "e\n",
            "t\n",
            "w\n",
            "o\n",
            "r\n",
            "k\n",
            "s\n",
            "#\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvQM17NKqddE",
        "outputId": "e9a26cee-9e00-4497-e3b5-4c7ec2e996a5"
      },
      "source": [
        "targets=[]\n",
        "print(\"letters is :\",letters)\n",
        "print(\"data is :\",data)\n",
        "\n",
        "for x in data[1:]+'#':\n",
        "  targets.append(letters.find(x))\n",
        "\n",
        "print(\"targets is :\",targets)\n",
        "print(\"\\nconverting it into tensors\")\n",
        "targets=torch.tensor(targets)\n",
        "print(targets)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "letters is : abcdefghijklmnopqrstuvwxyz #\n",
            "data is : i love neural networks\n",
            "targets is : [26, 11, 14, 21, 4, 26, 13, 4, 20, 17, 0, 11, 26, 13, 4, 19, 22, 14, 17, 10, 18, 27]\n",
            "\n",
            "converting it into tensors\n",
            "tensor([26, 11, 14, 21,  4, 26, 13,  4, 20, 17,  0, 11, 26, 13,  4, 19, 22, 14,\n",
            "        17, 10, 18, 27])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqrxPjjRspxD"
      },
      "source": [
        "This encoding is quite simple and straight forward. There are 28 allowed letters. (26 letters + 1 space + 1 ‘#’). \n",
        "\n",
        "I convert each of my character into a 28 dimensional vector. All the other dimensions have value 0, except the index of character in our string letters, which we defined above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqEDSh30ssVf",
        "outputId": "e3dd3339-31a3-4c00-bcd6-cebe6f3cd3ef"
      },
      "source": [
        "print(n_letters)\n",
        "print(letters)\n",
        "\n",
        "def ltt(ch):\n",
        "  ans=torch.zeros(n_letters)\n",
        "  ans[letters.find(ch)]=1\n",
        "  return ans\n",
        "\n",
        "print(\"\\n\\n\\n\")\n",
        "print(\"Encoding of 'b' \",ltt('b'))\n",
        "print(\"Encoding of '#' \",ltt('#'))  "
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "28\n",
            "abcdefghijklmnopqrstuvwxyz #\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Encoding of 'b'  tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Encoding of '#'  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4j7CqZxuNgR",
        "outputId": "d8243dd5-5ff7-45bc-b78a-6603a5bce13c"
      },
      "source": [
        "#Now, let’s build input sequence to train\n",
        "print(data)\n",
        "inpl=[]\n",
        "\n",
        "for c in data:\n",
        "  inpl.append(ltt(c))\n",
        "\n",
        "print(inpl)\n",
        "\n",
        "#Concatenates the given sequence of seq tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty.\n",
        "inp = torch.cat(inpl,dim=0)\n",
        "\n",
        "print(inp)\n",
        "\n",
        "#Reshape tensor into 3 dimensions (sequence length, batches = 1, dimensions = n_letters (28))\n",
        "inp=inp.view(seq_len,1,n_letters)\n",
        "\n",
        "print(\"shape  of inp\",inp.shape) # shape of inp torch.Size([22, 1, 28])\n",
        "#22 here becuase \"i love neural networks\" is of 22 length\n",
        "#1 becuase we are only passing one sentence in a batch\n",
        "#28 becuase , while 22 char's will be fed to network at 22 time steps ('i',' ','l','o','v','e' .......)all at once and each is reperesented by vector of length 28"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i love neural networks\n",
            "[tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.])\n",
            "shape  of inp torch.Size([22, 1, 28])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLU-520nw7Sr",
        "outputId": "bc03125d-fef8-4c05-c8b4-153cae89eb26"
      },
      "source": [
        "print(inp)\n",
        "#1st tensor below is for i\n",
        "#[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
        "#          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVvqF4LBymbT"
      },
      "source": [
        "TRAINING\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2_GbA3Syjkv",
        "outputId": "00fc5d1d-e024-4ccd-ae85-37043a3e42ef"
      },
      "source": [
        "import time\n",
        "start=time.time()\n",
        "n_iters=150\n",
        "\n",
        "for itr in range(n_iters):\n",
        "  model.zero_grad()\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  #initialse h and c\n",
        "  h=torch.rand(hidden_dim).view(1,1,hidden_dim)\n",
        "  c=torch.rand(hidden_dim).view(1,1,hidden_dim)\n",
        "  \n",
        "  #find output\n",
        "  output=model(inp,(h,c))\n",
        "\n",
        "  #Reshape the output to 2 dimensions. This is done, so that we can compare with target and get loss\n",
        "  output=output.view(seq_len,n_letters)\n",
        "  \n",
        "  #find loss\n",
        "  loss=LOSS(output,targets)\n",
        "  #backpropogate\n",
        "  loss.backward()\n",
        "  #optimise\n",
        "  optimizer.step\n",
        "\n",
        "print((time.time()-start))"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5509171485900879\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGMfCtYp1KKW",
        "outputId": "e4d0ba5b-7872-49b7-c054-7ea661e7ca61"
      },
      "source": [
        "print(output.shape)\n",
        "print(targets.shape)\n",
        "print(loss)\n",
        "print(targets)\n",
        "\n",
        "\n",
        "#    l  o v  e     n e u   r a  l    n  e  t w  o  r  k  s\n",
        "#26,11,14,21,4,26,13,4,20,17,0,11,26,13,4,19,22,14,17,10,18,27\n",
        "print(output)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([22, 28])\n",
            "torch.Size([22])\n",
            "tensor(3.3675, grad_fn=<NllLossBackward>)\n",
            "tensor([26, 11, 14, 21,  4, 26, 13,  4, 20, 17,  0, 11, 26, 13,  4, 19, 22, 14,\n",
            "        17, 10, 18, 27])\n",
            "tensor([[ 1.2021e-01,  1.9891e-01,  3.5588e-02,  1.0200e-02,  2.0669e-01,\n",
            "         -4.1973e-03,  6.9070e-02,  1.2849e-01,  1.5843e-01, -5.4210e-02,\n",
            "          2.3235e-01,  1.6107e-01, -3.0223e-03,  9.5466e-02,  1.7399e-01,\n",
            "         -1.2821e-02,  1.2816e-01,  6.1702e-02,  9.8040e-02,  8.0471e-02,\n",
            "          3.5173e-01,  2.7327e-01,  1.1794e-01,  4.1162e-01, -8.3781e-02,\n",
            "          9.2949e-02, -1.3487e-01,  2.1626e-01],\n",
            "        [ 1.1792e-01,  6.4241e-02,  3.9284e-02, -5.3765e-02,  8.5921e-02,\n",
            "          2.9360e-02,  1.1810e-01,  5.0615e-02,  1.6627e-01, -1.2917e-01,\n",
            "          7.5162e-02,  3.0393e-02,  2.6984e-02,  3.8626e-02,  2.8889e-02,\n",
            "         -4.0855e-02,  4.3367e-02,  6.3578e-02,  4.2251e-02, -4.2544e-02,\n",
            "          2.1858e-01,  9.9565e-02, -2.3334e-02,  2.8247e-01, -4.4822e-02,\n",
            "          3.3417e-03, -1.3140e-01,  1.4242e-01],\n",
            "        [ 1.1171e-01,  2.7653e-02, -2.0191e-02, -9.4013e-02,  1.0042e-02,\n",
            "          2.2677e-02,  8.8027e-02,  2.7446e-02,  1.0951e-01, -1.0605e-01,\n",
            "          1.8099e-02,  3.9561e-02,  4.3277e-02, -3.5728e-02, -1.1371e-02,\n",
            "         -4.8946e-02,  6.8883e-02,  2.1215e-02, -2.7368e-02, -8.5301e-02,\n",
            "          1.2056e-01, -1.1694e-03, -1.0234e-02,  1.8692e-01,  6.6725e-04,\n",
            "         -1.0202e-01, -9.4419e-02,  1.2755e-01],\n",
            "        [ 9.0628e-02, -2.2174e-02, -3.0146e-02, -1.2298e-01, -6.1869e-02,\n",
            "          4.5069e-02,  1.0449e-01,  2.5404e-03,  5.9031e-02, -9.1522e-02,\n",
            "         -1.4852e-02,  5.2972e-03,  9.8027e-02, -8.9665e-02,  4.5769e-03,\n",
            "         -3.8115e-02,  6.7948e-02,  7.0102e-04,  1.5483e-03, -1.1566e-01,\n",
            "          1.0149e-01, -3.5542e-02, -2.6298e-02,  1.4426e-01, -7.5566e-03,\n",
            "         -4.5419e-02, -1.2281e-01,  9.9774e-02],\n",
            "        [ 8.8738e-02, -7.2167e-02, -4.9050e-02, -1.0291e-01, -4.2524e-02,\n",
            "          2.4678e-02,  1.2460e-01, -6.1976e-02,  6.5341e-02, -3.8575e-02,\n",
            "         -1.8957e-02,  8.0994e-03,  7.9311e-02, -7.7462e-02,  7.3786e-03,\n",
            "         -4.4043e-03,  4.7022e-02,  2.4083e-02,  1.7637e-02, -1.3943e-01,\n",
            "          8.5899e-02, -1.3874e-02, -8.0992e-03,  7.0963e-02, -5.8897e-03,\n",
            "         -1.3704e-02, -1.1413e-01,  7.3170e-02],\n",
            "        [ 7.2969e-02, -1.0046e-01, -5.7925e-02, -1.0726e-01, -4.4147e-02,\n",
            "          3.5371e-02,  7.0777e-02, -5.6210e-02,  7.2523e-02, -1.3113e-02,\n",
            "         -3.0415e-02, -3.3769e-02,  9.3168e-02, -2.3055e-02,  2.1365e-02,\n",
            "         -2.1088e-02,  4.8854e-02, -3.1652e-02, -1.9352e-02, -6.5258e-02,\n",
            "          8.0646e-02, -6.3201e-02, -2.8994e-02,  8.5088e-02,  1.6669e-02,\n",
            "         -8.6637e-02, -1.1773e-01,  1.9591e-02],\n",
            "        [ 1.0255e-01, -7.5048e-02, -1.9637e-02, -9.0810e-02, -5.6566e-02,\n",
            "          6.7369e-02,  1.0682e-01, -5.3862e-02,  1.2216e-01, -8.0066e-02,\n",
            "         -1.0747e-01, -6.7576e-02,  8.2327e-02, -4.3955e-02, -4.5060e-02,\n",
            "         -5.4712e-03,  2.5837e-02,  1.2950e-02, -8.9485e-03, -9.3080e-02,\n",
            "          4.2836e-02, -3.9838e-02, -7.5933e-02,  9.0725e-02, -3.3885e-03,\n",
            "         -7.1919e-02, -1.1915e-01,  2.7089e-02],\n",
            "        [ 1.1073e-01, -6.0934e-02, -5.1421e-02, -1.0829e-01, -8.1590e-02,\n",
            "          9.6466e-02,  9.7523e-02, -4.7373e-02,  9.3612e-02, -3.7894e-02,\n",
            "         -1.2282e-01, -5.8621e-03,  1.0876e-01, -1.4696e-02, -1.7315e-02,\n",
            "         -7.1043e-03,  7.0188e-02, -3.9807e-02,  1.7434e-02, -4.9605e-02,\n",
            "          1.0006e-01,  4.7932e-03, -6.9049e-03,  4.2129e-02,  2.2539e-02,\n",
            "         -3.7853e-02, -3.5493e-02,  2.3751e-02],\n",
            "        [ 8.1091e-02, -1.0634e-01, -5.7522e-02, -1.2013e-01, -6.7794e-02,\n",
            "          6.3388e-02,  6.4509e-02, -5.2039e-02,  9.4491e-02, -6.6632e-03,\n",
            "         -8.0389e-02, -3.3724e-02,  1.0977e-01,  1.3095e-02,  2.4245e-02,\n",
            "         -1.6981e-02,  6.0941e-02, -7.5399e-02, -1.7183e-02,  1.2367e-03,\n",
            "          8.7227e-02, -5.2762e-02, -1.5000e-02,  7.2905e-02,  2.9525e-02,\n",
            "         -9.0757e-02, -8.8097e-02, -9.7165e-03],\n",
            "        [ 5.9960e-02, -1.0128e-01, -2.1976e-02, -1.4833e-01, -1.0876e-01,\n",
            "          8.2968e-02,  8.1962e-02, -2.2782e-02,  6.6901e-02, -7.3940e-02,\n",
            "         -1.1479e-01, -3.3598e-02,  1.0367e-01, -2.5216e-02, -2.3633e-02,\n",
            "          1.5746e-02,  7.1476e-02, -6.4813e-02,  1.5820e-02, -1.4249e-02,\n",
            "          7.7024e-02, -5.8816e-02, -6.0678e-02,  4.1047e-02,  5.9562e-02,\n",
            "         -7.7839e-02, -9.2317e-02, -2.3824e-02],\n",
            "        [ 8.8575e-02, -6.4780e-02, -2.0507e-02, -1.1491e-01, -1.3034e-01,\n",
            "          3.1052e-02,  7.5735e-02, -6.1843e-02,  9.1002e-02, -6.2336e-02,\n",
            "         -1.2301e-01,  1.8107e-03,  8.1204e-02, -6.7374e-02,  3.0753e-02,\n",
            "          1.2647e-02,  6.0784e-02, -2.2579e-02,  3.7751e-02, -3.9224e-02,\n",
            "          8.2979e-02, -8.2130e-02, -8.2596e-03,  7.3472e-02,  4.4583e-02,\n",
            "         -1.1277e-01, -7.2086e-02, -2.2156e-02],\n",
            "        [ 7.6153e-02, -3.9963e-02, -7.2962e-02, -6.4665e-02, -1.6346e-01,\n",
            "          4.0065e-02,  9.1130e-02, -1.0635e-01,  1.0086e-01, -2.6493e-02,\n",
            "         -1.5590e-01,  5.0654e-04,  8.8176e-02, -8.3664e-02, -4.5638e-02,\n",
            "         -1.1259e-02,  1.0841e-01, -1.8062e-02,  2.2192e-02, -7.9618e-03,\n",
            "          9.2546e-02, -3.4969e-02,  2.9759e-02,  5.4291e-02,  3.0813e-02,\n",
            "         -1.0761e-01, -4.0491e-02, -4.3017e-02],\n",
            "        [ 7.4899e-02, -4.6837e-02, -8.2174e-02, -8.9553e-02, -1.1084e-01,\n",
            "          4.9671e-02,  6.5224e-02, -6.4783e-02,  8.5402e-02, -4.0392e-02,\n",
            "         -1.1346e-01,  2.8556e-02,  7.6857e-02, -8.9995e-02, -4.5659e-02,\n",
            "         -1.6883e-02,  1.0673e-01, -4.7008e-02, -1.5900e-02, -4.7607e-02,\n",
            "          2.9588e-02, -6.8908e-02,  2.7864e-02,  3.0413e-02,  3.4465e-02,\n",
            "         -1.3232e-01, -6.8130e-02,  1.3741e-02],\n",
            "        [ 9.6714e-02, -5.2671e-02, -4.2237e-02, -7.8645e-02, -9.1839e-02,\n",
            "          7.5333e-02,  9.8015e-02, -5.7454e-02,  1.3404e-01, -9.9478e-02,\n",
            "         -1.5027e-01, -3.4743e-02,  7.3806e-02, -7.0274e-02, -8.0416e-02,\n",
            "          4.5487e-04,  4.6528e-02,  2.5000e-03, -4.3236e-03, -8.3754e-02,\n",
            "          1.3761e-02, -3.5806e-02, -4.3018e-02,  6.2865e-02, -6.0687e-04,\n",
            "         -8.7068e-02, -9.5079e-02,  1.3219e-02],\n",
            "        [ 1.0248e-01, -4.8279e-02, -6.4231e-02, -9.8642e-02, -9.7686e-02,\n",
            "          1.0107e-01,  9.0792e-02, -5.0161e-02,  1.0109e-01, -5.1407e-02,\n",
            "         -1.4219e-01,  1.4836e-02,  1.0368e-01, -2.2577e-02, -3.3727e-02,\n",
            "         -4.1825e-03,  8.0569e-02, -4.7511e-02,  2.1607e-02, -4.3953e-02,\n",
            "          8.4553e-02,  9.9386e-03,  1.2984e-02,  2.6426e-02,  2.3199e-02,\n",
            "         -4.1291e-02, -2.2484e-02,  1.2858e-02],\n",
            "        [ 7.4841e-02, -9.9638e-02, -6.5264e-02, -1.1336e-01, -7.7308e-02,\n",
            "          6.6498e-02,  5.9646e-02, -5.3964e-02,  1.0035e-01, -1.4315e-02,\n",
            "         -8.9409e-02, -2.1811e-02,  1.0779e-01,  9.9285e-03,  1.6500e-02,\n",
            "         -1.6490e-02,  6.6643e-02, -8.0737e-02, -1.4328e-02,  4.2598e-03,\n",
            "          7.7727e-02, -4.8462e-02, -1.9877e-03,  6.6424e-02,  2.9538e-02,\n",
            "         -9.0559e-02, -8.0735e-02, -1.6543e-02],\n",
            "        [ 4.0119e-02, -8.5299e-02, -1.7895e-02, -7.7868e-02, -1.1955e-01,\n",
            "          1.9835e-02,  9.9217e-02, -5.0439e-02,  1.3599e-01, -1.8118e-02,\n",
            "         -1.1425e-01, -6.3486e-02,  1.3318e-01, -2.0882e-02, -5.0250e-02,\n",
            "         -1.3619e-02,  5.3928e-02, -3.9538e-02, -3.2943e-02,  3.5730e-04,\n",
            "          1.1263e-01, -4.6698e-03,  7.0066e-03,  8.8697e-02,  1.4272e-02,\n",
            "         -2.6996e-02, -8.6611e-02, -1.2997e-02],\n",
            "        [ 9.1515e-02, -8.6426e-02, -7.0716e-03, -6.6862e-02, -1.0746e-01,\n",
            "          9.0581e-03,  1.3889e-01, -5.2575e-02,  1.0020e-01, -5.2629e-02,\n",
            "         -7.9666e-02, -3.4965e-02,  1.2292e-01, -3.2278e-02, -4.0672e-02,\n",
            "          1.6567e-02,  4.0620e-02, -2.9315e-02,  1.0131e-05, -2.7516e-02,\n",
            "          1.2255e-01,  2.8601e-02,  1.7002e-02,  6.1817e-02,  2.5485e-02,\n",
            "         -1.3747e-02, -1.3598e-01,  7.5764e-03],\n",
            "        [ 7.6840e-02, -8.6255e-02, -3.2603e-02, -1.0143e-01, -1.2364e-01,\n",
            "          4.8137e-02,  1.2483e-01, -3.4537e-02,  5.0483e-02, -2.7337e-02,\n",
            "         -7.5466e-02, -3.6547e-02,  1.3626e-01, -8.0657e-02, -1.9282e-02,\n",
            "         -5.6311e-03,  5.1336e-02, -3.3288e-02,  2.3231e-02, -8.3520e-02,\n",
            "          1.1567e-01, -4.0601e-02, -1.0618e-02,  6.4061e-02,  4.2954e-04,\n",
            "          8.8121e-03, -1.4013e-01,  4.6944e-02],\n",
            "        [ 9.0820e-02, -5.0806e-02, -2.2861e-02, -8.9988e-02, -1.4158e-01,\n",
            "          1.1545e-02,  9.4844e-02, -6.0659e-02,  7.3320e-02, -3.7224e-02,\n",
            "         -1.0792e-01, -7.3295e-03,  9.3423e-02, -9.6925e-02,  2.2191e-02,\n",
            "          1.2083e-03,  4.5130e-02,  4.8240e-03,  4.9588e-02, -9.1542e-02,\n",
            "          1.1113e-01, -7.2197e-02,  1.6911e-03,  8.6631e-02,  1.7116e-02,\n",
            "         -7.5234e-02, -9.4327e-02,  7.4474e-03],\n",
            "        [ 1.0481e-01, -9.9345e-02, -4.1273e-02, -1.0670e-01, -1.4726e-01,\n",
            "          7.3907e-02,  1.0326e-01, -8.7566e-02,  7.0368e-02, -2.5876e-02,\n",
            "         -1.0556e-01, -4.0628e-02,  1.2659e-01, -6.0745e-02, -4.1067e-02,\n",
            "          1.4062e-02,  4.7711e-02,  1.8793e-02,  3.0880e-02, -8.4033e-02,\n",
            "          1.1502e-01, -5.9741e-03,  8.5704e-03,  4.6320e-02,  4.6244e-02,\n",
            "         -6.2404e-02, -1.2467e-01, -4.8099e-03],\n",
            "        [ 1.2663e-01, -1.1349e-01, -1.5447e-02, -5.8426e-02, -1.1518e-01,\n",
            "          4.2249e-02,  7.6183e-02, -1.1722e-01,  7.4762e-02, -3.7065e-02,\n",
            "         -1.2144e-01, -2.8624e-02,  8.9693e-02, -8.2111e-02, -7.4704e-02,\n",
            "          4.5572e-02,  4.7163e-02,  1.2204e-02,  1.5185e-02, -6.1088e-02,\n",
            "          1.1076e-01,  1.7381e-02, -4.2731e-02,  8.8840e-02,  3.3115e-02,\n",
            "         -1.2491e-01, -8.8072e-02,  1.5172e-02]], grad_fn=<ViewBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUr2eMhe6aLJ"
      },
      "source": [
        "#Utility function that predicts next letter , given the sequence\n",
        "\n",
        "def getLine(s):\n",
        "    ans = []\n",
        "    for c in s:\n",
        "        ans.append(ltt(c))\n",
        "    return torch.cat(ans,dim=0).view(len(s),1,n_letters)\n",
        "    \n",
        "\n",
        "def predict(s):\n",
        "  inp=getLine(s)\n",
        "\n",
        "  h=torch.rand(1,1,hidden_dim)\n",
        "  c=torch.rand(1,1,hidden_dim)\n",
        "\n",
        "  out=model(inp,(h,c))\n",
        "\n",
        "  return letters[out[-1][0].topk(1)[1].detach().numpy().item()]##torch.topk ... Returns the k largest elements of the given input tensor along a given dimension k==1\n"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaZL4RT6iMGE"
      },
      "source": [
        "**combining everything**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yPwtH_ohhsI",
        "outputId": "df9537ae-a569-4880-fd1c-2f4369ec0f8a"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import string\n",
        "\n",
        "data = \"i love neural networks\"\n",
        "EOF = \"#\"\n",
        "#data = data+EOF\n",
        "data = data.lower()\n",
        "\n",
        "seq_len = len(data)\n",
        "\n",
        "letters = string.ascii_lowercase+' #'\n",
        "print('Letter set is '+letters)\n",
        "n_letters = len(letters)\n",
        "print(letters)\n",
        "\n",
        "#letter to tensor\n",
        "def ltt(ch):\n",
        "    ans = torch.zeros(n_letters)\n",
        "    ans[letters.find(ch)]=1\n",
        "    return ans\n",
        "    \n",
        "def getLine(s):\n",
        "    ans = []\n",
        "    for c in s:\n",
        "        ans.append(ltt(c))\n",
        "    return torch.cat(ans,dim=0).view(len(s),1,n_letters)\n",
        "    \n",
        "class MyLSTM(nn.Module):\n",
        "    def __init__(self,input_dim,hidden_dim):\n",
        "        super(MyLSTM,self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        #LSTM takes, input_dim, hidden_dim and num_layers incase of stacked LSTMs\n",
        "        self.LSTM = nn.LSTM(input_dim,hidden_dim)\n",
        "        self.LNN = nn.Linear(hidden_dim,input_dim)\n",
        "        \n",
        "    #Input must be 3 dimensional (seq_len, batch, input_dim). \n",
        "    #hc is a tuple of hidden and cell state vector. Each of them have shape (1,1,hidden_dim)\n",
        "    def forward(self,inp,hc):\n",
        "        #this gives outut for each input in the sequence and also (hidden and cell state vector)\n",
        "        #Dimensions of output vector is (seq_len,batch,hidden_dim)\n",
        "        output,_= self.LSTM(inp,hc)\n",
        "        return self.LNN(output)\n",
        "        #return output\n",
        "        \n",
        "\n",
        "#Dimensions of output of neural network is (seq_len, batch , hidden_dim). Since we want output dimensions to be\n",
        "#the same as n_letters, hidden_dim = n_letters (**output dimensions = hidden_dimensions)\n",
        "hidden_dim = n_letters     \n",
        "#Invoking model. Input dimensions = n_letters i.e 28. output dimensions = hidden_dimensions = 28\n",
        "model = MyLSTM(n_letters,hidden_dim)\n",
        "#I'm using Adam optimizer here\n",
        "optimizer = torch.optim.Adam(params = model.parameters(),lr=0.01)\n",
        "#Loss function is CrossEntropyLoss\n",
        "LOSS = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "#List to store targets\n",
        "targets = []\n",
        "#Iterate through all chars in the sequence, starting from second letter. Since output for 1st letter is the 2nd letter\n",
        "for x in data[1:]+'#':\n",
        "    #Find the target index. For a, it is 0, For 'b' it is 1 etc..\n",
        "    targets.append(letters.find(x))\n",
        "#Convert into tensor\n",
        "targets = torch.tensor(targets)\n",
        "    \n",
        "n_iters = 400\n",
        "\n",
        "#List to store input\n",
        "inpl = []\n",
        "#Iterate through all inputs in the sequence\n",
        "for c in data:\n",
        "    #Convert into tensor\n",
        "    inpl.append(ltt(c))\n",
        "#Convert list to tensor\n",
        "inp = torch.cat(inpl,dim=0)\n",
        "#Reshape tensor into 3 dimensions (sequence length, batches = 1, dimensions = n_letters (28))\n",
        "inp = inp.view(seq_len,1,n_letters)\n",
        "\n",
        "\n",
        "#Let's note down start time to track the training time\n",
        "import time\n",
        "start = time.time()\n",
        "#Number of iterations\n",
        "n_iters = 150\n",
        "for itr in range(n_iters):\n",
        "    #Zero the previosus gradients\n",
        "    model.zero_grad()\n",
        "    optimizer.zero_grad()\n",
        "    #Initialize h and c vectors\n",
        "    h = torch.rand(hidden_dim).view(1,1,hidden_dim)\n",
        "    c = torch.rand(hidden_dim).view(1,1,hidden_dim)\n",
        "    #Find the output\n",
        "    output = model(inp,(h,c))\n",
        "    #Reshape the output to 2 dimensions. This is done, so that we can compare with target and get loss\n",
        "    output = output.view(seq_len,n_letters)\n",
        "    #Find loss\n",
        "    loss = LOSS(output,targets)\n",
        "    #Print loss for every 10th iteration\n",
        "    if itr%10==0:\n",
        "        print('Iteration : '+str(itr)+' Loss : '+str(loss) )\n",
        "    #Back propagate the loss\n",
        "    loss.backward()\n",
        "    #Perform weight updation\n",
        "    optimizer.step()\n",
        "    \n",
        "print('Time taken to train : '+str(time.time()-start)+\" seconds\")\n",
        " \n",
        "\n",
        "#This utility method predicts the next letter given the sequence   \n",
        "def predict(s):\n",
        "    #Get the vector for input\n",
        "    inp = getLine(s)\n",
        "    #Initialize h and c vectors\n",
        "    h = torch.rand(1,1,hidden_dim)\n",
        "    c = torch.rand(1,1,hidden_dim)\n",
        "    #Get the output\n",
        "    print(\"input shape is {}\".format(inp.shape))\n",
        "    out = model(inp,(h,c))\n",
        "    #Find the corresponding letter from the output\n",
        "    return letters[out[-1][0].topk(1)[1].detach().numpy().item()]\n",
        "         "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Letter set is abcdefghijklmnopqrstuvwxyz #\n",
            "abcdefghijklmnopqrstuvwxyz #\n",
            "Iteration : 0 Loss : tensor(3.3682, grad_fn=<NllLossBackward>)\n",
            "Iteration : 10 Loss : tensor(2.5280, grad_fn=<NllLossBackward>)\n",
            "Iteration : 20 Loss : tensor(2.2620, grad_fn=<NllLossBackward>)\n",
            "Iteration : 30 Loss : tensor(1.8049, grad_fn=<NllLossBackward>)\n",
            "Iteration : 40 Loss : tensor(1.2759, grad_fn=<NllLossBackward>)\n",
            "Iteration : 50 Loss : tensor(0.7552, grad_fn=<NllLossBackward>)\n",
            "Iteration : 60 Loss : tensor(0.3560, grad_fn=<NllLossBackward>)\n",
            "Iteration : 70 Loss : tensor(0.1730, grad_fn=<NllLossBackward>)\n",
            "Iteration : 80 Loss : tensor(0.0861, grad_fn=<NllLossBackward>)\n",
            "Iteration : 90 Loss : tensor(0.0537, grad_fn=<NllLossBackward>)\n",
            "Iteration : 100 Loss : tensor(0.0385, grad_fn=<NllLossBackward>)\n",
            "Iteration : 110 Loss : tensor(0.0283, grad_fn=<NllLossBackward>)\n",
            "Iteration : 120 Loss : tensor(0.0224, grad_fn=<NllLossBackward>)\n",
            "Iteration : 130 Loss : tensor(0.0194, grad_fn=<NllLossBackward>)\n",
            "Iteration : 140 Loss : tensor(0.0177, grad_fn=<NllLossBackward>)\n",
            "Time taken to train : 0.9323925971984863 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UL9QxdGrh_Vs",
        "outputId": "549a3c61-c438-427a-d0f3-337a4a50289e"
      },
      "source": [
        "out=predict('i love ne')\n",
        "print(out)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input shape is torch.Size([9, 1, 28])\n",
            "u\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTMKxOPJiA2w",
        "outputId": "404ba671-29fe-4fb9-8407-7fd625db58d6"
      },
      "source": [
        "out=predict('i love neur')\n",
        "print(out)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input shape is torch.Size([11, 1, 28])\n",
            "a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6Pdmq4liBiV",
        "outputId": "febb3a96-e37c-4bd8-e5f6-cb0b7d6eae8c"
      },
      "source": [
        "out=predict('i love neural ne')\n",
        "print(out)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjvVK3h1iCIl",
        "outputId": "e707eece-aae0-4568-e864-7f58d5688554"
      },
      "source": [
        "out=predict('i lov')\n",
        "print(out)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "e\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}